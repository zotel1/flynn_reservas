import type { VercelRequest, VercelResponse } from '@vercel/node';

// Historial limitado a las √∫ltimas 8 interacciones
let conversationHistory: { role: string; parts: { text: string }[] }[] = [];

module.exports = async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'M√©todo no permitido' });
  }

  const { message, history } = req.body || {};
  if (!message || typeof message !== 'string') {
    return res.status(400).json({ error: 'Mensaje vac√≠o o inv√°lido' });
  }

  // Permitir reiniciar conversaci√≥n
  if (message.toLowerCase().includes('reiniciar') || message.toLowerCase().includes('borrar')) {
    conversationHistory = [];
    return res.status(200).json({
      reply: 'Conversaci√≥n reiniciada üçÄ ¬°Empecemos de nuevo!',
    });
  }

  const GEMINI_API_KEY = process.env['GEMINI_API_KEY'] as string | undefined;
  if (!GEMINI_API_KEY) {
    return res.status(500).json({ error: 'Falta GEMINI_API_KEY en el entorno' });
  }

  try {
    // === PERSONALIDAD DEL BOT ===
    const systemPrompt = `
Sos Flynn Assistant üçÄ, el asistente virtual del Flynn Irish Pub en Posadas, Misiones.
Tu estilo es c√°lido, cercano y con acento del litoral argentino.
Respond√© con tono simp√°tico, como un amigo del bar.
S√© breve (m√°x. 2 frases) y respond√© en espa√±ol.
Si te preguntan por reservas, dec√≠ que pueden hacerlas desde el sitio.
Si te preguntan algo fuera del contexto del bar, respond√©: "Perd√≥n üçÄ, eso no lo s√©, pero puedo contarte sobre el bar o sus eventos."
Usuario dice: "${message}"
`.trim();

    // Actualizar historial (m√°x. 8 mensajes)
    const recentMessages = (history || [])
      .slice(-8)
      .map((m: any) => ({
        role: m.isBot ? 'model' : 'user',
        parts: [{ text: m.text }],
      }));

    // Agregar mensaje actual
    recentMessages.push({ role: 'user', parts: [{ text: message }] });
    conversationHistory = [...conversationHistory, ...recentMessages].slice(-8);

    // Endpoint actualizado a Gemini 2.5 Flash
    const endpoint = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${GEMINI_API_KEY}`;

    const response = await fetch(endpoint, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents: [
          { role: 'user', parts: [{ text: systemPrompt }] },
          ...conversationHistory,
        ],
      }),
    });

    if (!response.ok) {
      const text = await response.text();
      console.error('‚ùå Error en Gemini:', text);
      return res.status(response.status).json({ error: text });
    }

    const data = await response.json();
    const reply =
      data?.candidates?.[0]?.content?.parts?.[0]?.text ??
      'No pude generar una respuesta üçÄ';

    // Guardar respuesta en historial
    conversationHistory.push({ role: 'model', parts: [{ text: reply }] });
    conversationHistory = conversationHistory.slice(-8);

    return res.status(200).json({ reply });
  } catch (err: any) {
    console.error('üî• Error interno:', err);
    return res.status(500).json({ error: err.message || 'Error interno del servidor' });
  }
};
